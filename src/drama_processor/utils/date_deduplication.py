"""åŸºäºæ—¥æœŸçš„å‰§é›†å»é‡ç®¡ç†å™¨"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

logger = logging.getLogger(__name__)


class DateDeduplicationManager:
    """åŸºäºæ—¥æœŸçš„å‰§é›†å»é‡ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    - è®°å½•æ¯ä¸ªæ—¥æœŸå·²ç»å¤„ç†è¿‡çš„å‰§é›†
    - åœ¨æ‹‰å–é£ä¹¦æ•°æ®æ—¶è¿‡æ»¤å·²å¤„ç†çš„å‰§é›†
    - æ”¯æŒå¼ºåˆ¶é‡æ–°å¤„ç†é€‰é¡¹
    """
    
    def __init__(self, base_dir: Optional[Path] = None):
        """åˆå§‹åŒ–æ—¥æœŸå»é‡ç®¡ç†å™¨
        
        Args:
            base_dir: å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ history/date_dedup/
        """
        if base_dir is None:
            # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
            current = Path.cwd()
            while current != current.parent:
                if (current / "pyproject.toml").exists():
                    base_dir = current / "history" / "date_dedup"
                    break
                current = current.parent
            else:
                base_dir = Path.cwd() / "history" / "date_dedup"
        
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        logger.debug(f"æ—¥æœŸå»é‡ç®¡ç†å™¨åˆå§‹åŒ–ï¼Œå­˜å‚¨ç›®å½•: {self.base_dir}")
    
    def _get_date_file(self, date_str: str) -> Path:
        """è·å–æ—¥æœŸå¯¹åº”çš„å­˜å‚¨æ–‡ä»¶è·¯å¾„
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12"
            
        Returns:
            å­˜å‚¨æ–‡ä»¶è·¯å¾„
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼šå°† "9.12" è½¬æ¢ä¸º "09-12"
        normalized_date = self._normalize_date_str(date_str)
        return self.base_dir / f"{normalized_date}.json"
    
    def _normalize_date_str(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸå­—ç¬¦ä¸²æ ¼å¼
        
        Args:
            date_str: åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12", "09.12", "9-12" ç­‰
            
        Returns:
            æ ‡å‡†åŒ–æ ¼å¼ "09-12"
        """
        # å¤„ç†å„ç§å¯èƒ½çš„è¾“å…¥æ ¼å¼
        if '.' in date_str:
            parts = date_str.split('.')
        elif '-' in date_str:
            parts = date_str.split('-')
        else:
            # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œå‡è®¾æ˜¯æœˆæ—¥æ ¼å¼ï¼ˆéœ€è¦è‡³å°‘2ä½æ•°å­—ï¼‰
            if len(date_str) >= 2:
                parts = [date_str[:-2], date_str[-2:]]
            else:
                logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
                return date_str.replace('.', '-')
        
        if len(parts) == 2:
            month, day = parts
            try:
                # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                month_int = int(month)
                day_int = int(day)
                return f"{month_int:02d}-{day_int:02d}"
            except ValueError:
                logger.warning(f"æ—¥æœŸæ ¼å¼æ— æ•ˆ: {date_str}")
                return date_str.replace('.', '-')
        
        logger.warning(f"æ—¥æœŸæ ¼å¼æ— æ³•è¯†åˆ«: {date_str}")
        return date_str.replace('.', '-')
    
    def load_processed_dramas(self, date_str: str) -> Set[str]:
        """åŠ è½½æŒ‡å®šæ—¥æœŸå·²å¤„ç†çš„å‰§é›†åˆ—è¡¨
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12"
            
        Returns:
            å·²å¤„ç†çš„å‰§é›†åç§°é›†åˆ
        """
        date_file = self._get_date_file(date_str)
        
        if not date_file.exists():
            logger.debug(f"æ—¥æœŸ {date_str} æ— å†å²å¤„ç†è®°å½•")
            return set()
        
        try:
            with open(date_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            processed_dramas = set(data.get('processed_dramas', []))
            logger.info(f"ğŸ“… æ—¥æœŸ {date_str}: åŠ è½½äº† {len(processed_dramas)} ä¸ªå·²å¤„ç†å‰§é›†")
            return processed_dramas
            
        except Exception as e:
            logger.error(f"åŠ è½½æ—¥æœŸ {date_str} çš„å¤„ç†è®°å½•å¤±è´¥: {e}")
            return set()
    
    def save_processed_dramas(self, date_str: str, drama_names: List[str]):
        """ä¿å­˜æŒ‡å®šæ—¥æœŸçš„å·²å¤„ç†å‰§é›†åˆ—è¡¨
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12"
            drama_names: æ–°å¤„ç†çš„å‰§é›†åç§°åˆ—è¡¨
        """
        if not drama_names:
            logger.debug(f"æ—¥æœŸ {date_str} æ— æ–°å¤„ç†å‰§é›†ï¼Œè·³è¿‡ä¿å­˜")
            return
        
        # åŠ è½½ç°æœ‰è®°å½•
        existing_dramas = self.load_processed_dramas(date_str)
        
        # åˆå¹¶æ–°æ—§è®°å½•
        all_dramas = existing_dramas.union(set(drama_names))
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        data = {
            'date': date_str,
            'normalized_date': self._normalize_date_str(date_str),
            'last_updated': datetime.now().isoformat(),
            'processed_dramas': sorted(list(all_dramas)),
            'total_count': len(all_dramas)
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        date_file = self._get_date_file(date_str)
        try:
            with open(date_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            new_count = len(set(drama_names) - existing_dramas)
            logger.info(f"ğŸ’¾ æ—¥æœŸ {date_str}: ä¿å­˜äº† {new_count} ä¸ªæ–°å¤„ç†å‰§é›†ï¼ˆæ€»è®¡ {len(all_dramas)} ä¸ªï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ—¥æœŸ {date_str} çš„å¤„ç†è®°å½•å¤±è´¥: {e}")
    
    def filter_new_dramas(self, drama_info: Dict[str, Dict[str, str]], 
                         force_reprocess: bool = False) -> Tuple[Dict[str, Dict[str, str]], List[str]]:
        """è¿‡æ»¤å‡ºæœªå¤„ç†çš„å‰§é›†
        
        Args:
            drama_info: å‰§é›†ä¿¡æ¯å­—å…¸ï¼Œæ ¼å¼ä¸º {å‰§å: {"record_id": "xxx", "date": "9.12"}}
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†ï¼ˆå¿½ç•¥å†å²è®°å½•ï¼‰
            
        Returns:
            (è¿‡æ»¤åçš„å‰§é›†ä¿¡æ¯, è¢«è·³è¿‡çš„å‰§é›†åˆ—è¡¨)
        """
        if force_reprocess:
            logger.info("ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼ï¼Œè·³è¿‡å»é‡æ£€æŸ¥")
            return drama_info, []
        
        if not drama_info:
            return {}, []
        
        # æŒ‰æ—¥æœŸåˆ†ç»„å¤„ç†
        date_groups = {}
        for drama_name, info in drama_info.items():
            date_str = info.get('date', 'æœªçŸ¥')
            if date_str not in date_groups:
                date_groups[date_str] = {}
            date_groups[date_str][drama_name] = info
        
        filtered_dramas = {}
        skipped_dramas = []
        
        for date_str, dramas_for_date in date_groups.items():
            if date_str == 'æœªçŸ¥':
                # å¯¹äºæœªçŸ¥æ—¥æœŸçš„å‰§é›†ï¼Œä¸è¿›è¡Œå»é‡
                logger.warning(f"å‘ç° {len(dramas_for_date)} ä¸ªæœªçŸ¥æ—¥æœŸçš„å‰§é›†ï¼Œè·³è¿‡å»é‡")
                filtered_dramas.update(dramas_for_date)
                continue
            
            # åŠ è½½è¯¥æ—¥æœŸå·²å¤„ç†çš„å‰§é›†
            processed_dramas = self.load_processed_dramas(date_str)
            
            # è¿‡æ»¤è¯¥æ—¥æœŸçš„å‰§é›†
            for drama_name, info in dramas_for_date.items():
                if drama_name in processed_dramas:
                    skipped_dramas.append(drama_name)
                    logger.info(f"â­ï¸  è·³è¿‡å·²å¤„ç†å‰§é›†: {drama_name} (æ—¥æœŸ: {date_str})")
                else:
                    filtered_dramas[drama_name] = info
                    logger.debug(f"âœ… æ–°å‰§é›†å¾…å¤„ç†: {drama_name} (æ—¥æœŸ: {date_str})")
        
        # ç»Ÿè®¡ä¿¡æ¯
        original_count = len(drama_info)
        filtered_count = len(filtered_dramas)
        skipped_count = len(skipped_dramas)
        
        if skipped_count > 0:
            logger.info(f"ğŸ“Š å»é‡ç»“æœ: åŸå§‹ {original_count} éƒ¨ -> è¿‡æ»¤å {filtered_count} éƒ¨ (è·³è¿‡ {skipped_count} éƒ¨)")
        else:
            logger.info(f"ğŸ“Š å»é‡ç»“æœ: {original_count} éƒ¨å‰§é›†å‡ä¸ºæ–°å‰§é›†")
        
        return filtered_dramas, skipped_dramas
    
    def mark_dramas_as_processed(self, drama_results: List[Dict[str, any]]):
        """å°†æˆåŠŸå¤„ç†çš„å‰§é›†æ ‡è®°ä¸ºå·²å¤„ç†
        
        Args:
            drama_results: å‰§é›†å¤„ç†ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {name, date, status, completed, planned} ç­‰
        """
        # æŒ‰æ—¥æœŸåˆ†ç»„
        date_groups = {}
        for result in drama_results:
            drama_name = result.get('name')
            date_str = result.get('date', 'æœªçŸ¥')
            
            # åªæ ‡è®°å®Œæˆçš„å‰§é›†
            completed = result.get('completed', 0)
            planned = result.get('planned', 0)
            
            if completed > 0:  # è‡³å°‘å®Œæˆäº†ä¸€äº›ç´ æ
                if date_str not in date_groups:
                    date_groups[date_str] = []
                date_groups[date_str].append(drama_name)
                logger.debug(f"æ ‡è®°å‰§é›†ä¸ºå·²å¤„ç†: {drama_name} (æ—¥æœŸ: {date_str}, å®Œæˆ: {completed}/{planned})")
        
        # ä¿å­˜å„æ—¥æœŸçš„å¤„ç†è®°å½•
        for date_str, drama_names in date_groups.items():
            if date_str != 'æœªçŸ¥':
                self.save_processed_dramas(date_str, drama_names)
    
    def get_date_summary(self, date_str: str) -> Optional[Dict[str, any]]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„å¤„ç†æ‘˜è¦
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12"
            
        Returns:
            å¤„ç†æ‘˜è¦ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰è®°å½•è¿”å› None
        """
        date_file = self._get_date_file(date_str)
        
        if not date_file.exists():
            return None
        
        try:
            with open(date_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return {
                'date': data.get('date'),
                'processed_count': data.get('total_count', 0),
                'last_updated': data.get('last_updated'),
                'processed_dramas': data.get('processed_dramas', [])
            }
            
        except Exception as e:
            logger.error(f"è¯»å–æ—¥æœŸ {date_str} æ‘˜è¦å¤±è´¥: {e}")
            return None
    
    def list_all_processed_dates(self) -> List[Dict[str, any]]:
        """åˆ—å‡ºæ‰€æœ‰æœ‰å¤„ç†è®°å½•çš„æ—¥æœŸ
        
        Returns:
            æ—¥æœŸæ‘˜è¦åˆ—è¡¨ï¼ŒæŒ‰æ—¥æœŸæ’åº
        """
        summaries = []
        
        for date_file in self.base_dir.glob("*.json"):
            try:
                with open(date_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                summaries.append({
                    'date': data.get('date'),
                    'normalized_date': data.get('normalized_date'),
                    'processed_count': data.get('total_count', 0),
                    'last_updated': data.get('last_updated'),
                    'file_path': str(date_file)
                })
                
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {date_file}: {e}")
        
        # æŒ‰æ ‡å‡†åŒ–æ—¥æœŸæ’åº
        summaries.sort(key=lambda x: x.get('normalized_date', ''))
        return summaries
    
    def clear_date_record(self, date_str: str) -> bool:
        """æ¸…é™¤æŒ‡å®šæ—¥æœŸçš„å¤„ç†è®°å½•
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "9.12"
            
        Returns:
            æ˜¯å¦æˆåŠŸæ¸…é™¤
        """
        date_file = self._get_date_file(date_str)
        
        if not date_file.exists():
            logger.warning(f"æ—¥æœŸ {date_str} æ— å¤„ç†è®°å½•ï¼Œæ— éœ€æ¸…é™¤")
            return False
        
        try:
            date_file.unlink()
            logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤æ—¥æœŸ {date_str} çš„å¤„ç†è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…é™¤æ—¥æœŸ {date_str} è®°å½•å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_date_dedup_manager = None


def get_date_dedup_manager() -> DateDeduplicationManager:
    """è·å–å…¨å±€æ—¥æœŸå»é‡ç®¡ç†å™¨å®ä¾‹"""
    global _date_dedup_manager
    if _date_dedup_manager is None:
        _date_dedup_manager = DateDeduplicationManager()
    return _date_dedup_manager



"""æ™ºèƒ½åœºæ™¯è¯†åˆ«ä¸å‰ªè¾‘ç‚¹æ£€æµ‹æ¨¡å—"""

import cv2
import numpy as np
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Optional, NamedTuple
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)


@dataclass
class SceneInfo:
    """åœºæ™¯ä¿¡æ¯"""
    start_time: float      # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float        # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    quality_score: float   # è´¨é‡è¯„åˆ† (0-1)
    scene_type: str        # åœºæ™¯ç±»å‹
    key_frame_path: Optional[str] = None  # å…³é”®å¸§è·¯å¾„
    confidence: float = 0.0  # ç½®ä¿¡åº¦


@dataclass
class OptimalCutPoint:
    """æœ€ä½³å‰ªè¾‘ç‚¹"""
    timestamp: float       # æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    confidence: float      # ç½®ä¿¡åº¦ (0-1)
    cut_type: str         # å‰ªè¾‘ç‚¹ç±»å‹: 'scene_change', 'dialogue_pause', 'action_peak'


class SceneAnalyzer:
    """æ™ºèƒ½åœºæ™¯åˆ†æå™¨
    
    ===== AIå‰ªè¾‘ç‚¹ç®—æ³•è¯´æ˜ =====
    
    1. åœºæ™¯æ£€æµ‹ç®—æ³•ï¼š
       - åŸºäºè§†é¢‘å¸§ç›´æ–¹å›¾ç›¸å…³æ€§æ£€æµ‹åœºæ™¯å˜åŒ–
       - é˜ˆå€¼ï¼š30% (correlation < 0.7 æ—¶è®¤ä¸ºåœºæ™¯å˜åŒ–)
       - æœ€å°åœºæ™¯æ—¶é•¿ï¼š2ç§’ (é¿å…è¯¯æ£€æµ‹)
    
    2. å‰ªè¾‘ç‚¹è¯„åˆ†ç®—æ³•ï¼š
       - ç½®ä¿¡åº¦ (70%æƒé‡): åŸºäºåœºæ™¯å˜åŒ–å¼ºåº¦ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºåœºæ™¯å˜åŒ–è¶Šæ˜æ˜¾
       - æ—¶é—´ä½ç½®åå¥½ (30%æƒé‡): åå¥½30ç§’å·¦å³ä½ç½®ï¼Œé¿å…ç‰‡å¤´ç‰‡å°¾
    
    3. ç­›é€‰ç­–ç•¥ï¼š
       - é¿å¼€å‰3ç§’ (ç‰‡å¤´/å¹¿å‘Š)
       - é¿å¼€å15ç§’ (ç¡®ä¿æ‰©å±•ç©ºé—´)
       - ç½®ä¿¡åº¦è¦æ±‚ â‰¥ 0.6
       - è¿”å›è¯„åˆ†æœ€é«˜çš„1ä¸ªèµ·å§‹ç‚¹
    
    4. ä½œç”¨è¯´æ˜ï¼š
       - ç½®ä¿¡åº¦ï¼šç¡®ä¿å‰ªè¾‘ç‚¹åœ¨çœŸå®çš„åœºæ™¯è¾¹ç•Œï¼Œé¿å…åœ¨åŒä¸€åœºæ™¯ä¸­é—´åˆ‡æ–­
       - æ—¶é—´ä½ç½®åå¥½ï¼šé¿å…ç‰‡å¤´å¹¿å‘Šå’Œç‰‡å°¾é¢„å‘Šï¼Œé€‰æ‹©å‰§æƒ…æ­£å¼å¼€å§‹çš„ä½ç½®
    """
    
    def __init__(self, model_dir: Optional[Path] = None):
        """åˆå§‹åŒ–åœºæ™¯åˆ†æå™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        """
        self.model_dir = model_dir or Path.home() / ".drama_processor" / "models"
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # åœºæ™¯æ£€æµ‹å‚æ•°
        self.scene_threshold = 30.0  # åœºæ™¯å˜åŒ–é˜ˆå€¼
        self.min_scene_duration = 2.0  # æœ€å°åœºæ™¯æ—¶é•¿ï¼ˆç§’ï¼‰
        
        logger.info("åœºæ™¯åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_video_scenes(self, video_path: Path, 
                           sample_rate: float = 1.0) -> List[SceneInfo]:
        """åˆ†æè§†é¢‘åœºæ™¯
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ï¼ˆæ¯ç§’é‡‡æ ·å¸§æ•°ï¼‰
            
        Returns:
            åœºæ™¯ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹åˆ†æè§†é¢‘åœºæ™¯: {video_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        logger.info(f"æ–‡ä»¶å¤§å°: {video_path.stat().st_size / (1024*1024):.1f} MB")
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        logger.info(f"è§†é¢‘ä¿¡æ¯: {fps:.2f}fps, {total_frames}å¸§, {duration:.2f}ç§’")
        
        # æ£€æŸ¥è§†é¢‘å‚æ•°æ˜¯å¦åˆç†
        if fps <= 0 or total_frames <= 0:
            cap.release()
            raise RuntimeError(f"è§†é¢‘å‚æ•°å¼‚å¸¸: fps={fps}, frames={total_frames}")
        
        # å¯¹äºå¾ˆé•¿çš„è§†é¢‘ï¼Œé™ä½é‡‡æ ·ç‡ä»¥é¿å…å¡æ­»
        if duration > 3600:  # è¶…è¿‡1å°æ—¶
            logger.warning(f"è§†é¢‘æ—¶é•¿è¿‡é•¿ ({duration/60:.1f}åˆ†é’Ÿ)ï¼Œè‡ªåŠ¨é™ä½é‡‡æ ·ç‡")
            sample_rate = min(sample_rate, 0.2)  # æœ€å¤šæ¯5ç§’é‡‡æ ·ä¸€æ¬¡
        elif duration > 1800:  # è¶…è¿‡30åˆ†é’Ÿ
            sample_rate = min(sample_rate, 0.5)  # æœ€å¤šæ¯2ç§’é‡‡æ ·ä¸€æ¬¡
        
        logger.info(f"å®é™…é‡‡æ ·ç‡: {sample_rate:.2f} fps")
        
        try:
            # åœºæ™¯æ£€æµ‹
            scenes = self._detect_scenes(cap, fps, sample_rate)
            
            # ç²¾ç®€å¤„ç†ï¼šç»™æ‰€æœ‰åœºæ™¯è®¾ç½®é»˜è®¤åˆ†æ•°å’Œç±»å‹
            for scene in scenes:
                scene.quality_score = 0.8  # é»˜è®¤è´¨é‡åˆ†æ•°ï¼ˆä¸å†è¿›è¡Œå¤æ‚è´¨é‡è¯„ä¼°ï¼‰
                scene.scene_type = "scene_change"  # ç»Ÿä¸€åœºæ™¯ç±»å‹
            
            logger.info(f"âœ… æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")
            return scenes
            
        except Exception as e:
            logger.error(f"âŒ åœºæ™¯æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤åœºæ™¯ï¼Œé¿å…å®Œå…¨å¤±è´¥
            default_scene = SceneInfo(
                start_time=0.0,
                end_time=duration,
                quality_score=0.5,
                scene_type="default"
            )
            return [default_scene]
        finally:
            cap.release()
    
    def find_optimal_cut_points(self, video_path: Path, 
                              target_duration: float,
                              min_duration: float = 300.0,
                              max_duration: float = 900.0) -> List[OptimalCutPoint]:
        """æ‰¾åˆ°æœ€ä½³å‰ªè¾‘ç‚¹
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰
            min_duration: æœ€å°æ—¶é•¿
            max_duration: æœ€å¤§æ—¶é•¿
            
        Returns:
            æœ€ä½³å‰ªè¾‘ç‚¹åˆ—è¡¨
        """
        logger.info(f"å¯»æ‰¾æœ€ä½³å‰ªè¾‘ç‚¹: ç›®æ ‡{target_duration}s, èŒƒå›´{min_duration}-{max_duration}s")
        
        # 1. è·å–åœºæ™¯ä¿¡æ¯
        scenes = self.analyze_video_scenes(video_path)
        
        # 2. æ£€æµ‹åœºæ™¯å˜åŒ–ç‚¹
        scene_changes = self._extract_scene_change_points(scenes)
        
        # 3. ç®€åŒ–å¤„ç†ï¼šåªä½¿ç”¨åœºæ™¯å˜åŒ–ç‚¹
        # dialogue_pauses = self._detect_dialogue_pauses(video_path)  # æš‚æ—¶ç¦ç”¨
        # action_peaks = self._detect_action_peaks(video_path)  # æš‚æ—¶ç¦ç”¨
        
        # 4. ç»¼åˆè¯„ä¼°æœ€ä½³å‰ªè¾‘ç‚¹
        all_cut_points = scene_changes  # åªä½¿ç”¨åœºæ™¯å˜åŒ–ç‚¹
        optimal_points = self._select_optimal_segments(
            all_cut_points, target_duration, min_duration, max_duration
        )
        
        logger.info(f"æ‰¾åˆ° {len(optimal_points)} ä¸ªæœ€ä½³å‰ªè¾‘ç‚¹")
        return optimal_points
    
    def _detect_scenes(self, cap: cv2.VideoCapture, fps: float, 
                      sample_rate: float) -> List[SceneInfo]:
        """æ£€æµ‹åœºæ™¯å˜åŒ–"""
        scenes = []
        prev_hist = None
        scene_start = 0.0
        frame_interval = int(fps / sample_rate)
        
        frame_count = 0
        processed_frames = 0
        max_process_frames = 3000  # æœ€å¤šå¤„ç†3000å¸§ï¼Œé¿å…å¡æ­»
        
        logger.info(f"å¼€å§‹åœºæ™¯æ£€æµ‹ï¼Œé‡‡æ ·é—´éš”: {frame_interval} å¸§ï¼Œæœ€å¤§å¤„ç†å¸§æ•°: {max_process_frames}")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                logger.info(f"ğŸ“¹ è§†é¢‘è¯»å–å®Œæˆï¼Œæ€»å…±å¤„ç†äº† {processed_frames} å¸§")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤„ç†å¸§æ•°
            if processed_frames >= max_process_frames:
                logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å¤„ç†å¸§æ•°é™åˆ¶ ({max_process_frames})ï¼Œåœæ­¢å¤„ç†")
                break
            
            # æŒ‰é‡‡æ ·ç‡å¤„ç†å¸§
            if frame_count % frame_interval != 0:
                frame_count += 1
                continue
            
            processed_frames += 1
            if processed_frames % 100 == 0:  # æ¯å¤„ç†100å¸§è¾“å‡ºä¸€æ¬¡è¿›åº¦
                progress_pct = (processed_frames / max_process_frames) * 100
                logger.info(f"ğŸ”„ å·²å¤„ç† {processed_frames} å¸§ ({progress_pct:.1f}%)ï¼Œå½“å‰æ—¶é—´: {frame_count / fps:.1f}ç§’")
            
            current_time = frame_count / fps
            
            # è®¡ç®—ç›´æ–¹å›¾
            hist = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            hist = cv2.normalize(hist, hist).flatten()
            
            if prev_hist is not None:
                # è®¡ç®—ç›´æ–¹å›¾å·®å¼‚
                correlation = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL)
                
                # åœºæ™¯å˜åŒ–æ£€æµ‹
                if correlation < (1 - self.scene_threshold / 100.0):
                    # æ£€æµ‹åˆ°åœºæ™¯å˜åŒ–
                    if current_time - scene_start >= self.min_scene_duration:
                        scene = SceneInfo(
                            start_time=scene_start,
                            end_time=current_time,
                            quality_score=0.0,  # åç»­è®¡ç®—
                            scene_type="unknown"
                        )
                        scenes.append(scene)
                        scene_start = current_time
            
            prev_hist = hist
            frame_count += 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªåœºæ™¯
        if scene_start < frame_count / fps:
            final_scene = SceneInfo(
                start_time=scene_start,
                end_time=frame_count / fps,
                quality_score=0.0,
                scene_type="unknown"
            )
            scenes.append(final_scene)
        
        return scenes
    
    # ç§»é™¤è´¨é‡è¯„ä¼°ç›¸å…³æ–¹æ³• - ä¸å†éœ€è¦
    
    def _extract_scene_change_points(self, scenes: List[SceneInfo]) -> List[OptimalCutPoint]:
        """ä»åœºæ™¯ä¸­æå–åœºæ™¯å˜åŒ–å‰ªè¾‘ç‚¹"""
        cut_points = []
        
        for i, scene in enumerate(scenes[:-1]):  # ä¸åŒ…æ‹¬æœ€åä¸€ä¸ªåœºæ™¯
            next_scene = scenes[i + 1]
            
            # åœºæ™¯å˜åŒ–ç‚¹å°±æ˜¯å½“å‰åœºæ™¯ç»“æŸæ—¶é—´
            cut_point = OptimalCutPoint(
                timestamp=scene.end_time,
                confidence=0.8,  # åœºæ™¯å˜åŒ–ç‚¹é€šå¸¸æ¯”è¾ƒå¯é 
                cut_type="scene_change"
            )
            cut_points.append(cut_point)
        
        return cut_points
    
    def _detect_dialogue_pauses(self, video_path: Path) -> List[OptimalCutPoint]:
        """æ£€æµ‹å¯¹è¯åœé¡¿ç‚¹ï¼ˆåŸºäºéŸ³é¢‘åˆ†æï¼‰"""
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯ä»¥ä½¿ç”¨librosaè¿›è¡ŒéŸ³é¢‘åˆ†æ
        cut_points = []
        
        try:
            import librosa
            
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(str(video_path), sr=None)
            
            # æ£€æµ‹é™éŸ³æ®µ
            intervals = librosa.effects.split(y, top_db=20)  # 20dBé˜ˆå€¼
            
            # åœ¨é™éŸ³æ®µä¹‹é—´æ‰¾åœé¡¿ç‚¹
            for i in range(len(intervals) - 1):
                end_time = intervals[i][1] / sr
                start_time = intervals[i + 1][0] / sr
                
                # å¦‚æœé™éŸ³æ—¶é—´è¶…è¿‡0.5ç§’ï¼Œè®¤ä¸ºæ˜¯å¯¹è¯åœé¡¿
                if start_time - end_time > 0.5:
                    cut_point = OptimalCutPoint(
                        timestamp=(end_time + start_time) / 2,
                        confidence=0.6,
                        cut_type="dialogue_pause"
                    )
                    cut_points.append(cut_point)
        
        except ImportError:
            logger.warning("librosaæœªå®‰è£…ï¼Œè·³è¿‡å¯¹è¯åœé¡¿æ£€æµ‹")
        except Exception as e:
            logger.warning(f"å¯¹è¯åœé¡¿æ£€æµ‹å¤±è´¥: {e}")
        
        return cut_points
    
    def _detect_action_peaks(self, video_path: Path) -> List[OptimalCutPoint]:
        """æ£€æµ‹åŠ¨ä½œé«˜æ½®ç‚¹ï¼ˆåŸºäºè¿åŠ¨åˆ†æï¼‰"""
        cut_points = []
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            return cut_points
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        prev_frame = None
        motion_scores = []
        timestamps = []
        
        frame_count = 0
        sample_interval = int(fps)  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % sample_interval == 0:
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                if prev_frame is not None:
                    # è®¡ç®—å…‰æµæ¥æ£€æµ‹è¿åŠ¨
                    flow = cv2.calcOpticalFlowPyrLK(
                        prev_frame, gray, 
                        np.array([[100, 100]], dtype=np.float32).reshape(-1, 1, 2),
                        None
                    )[0]
                    
                    # è®¡ç®—è¿åŠ¨å¼ºåº¦
                    if flow is not None and len(flow) > 0:
                        motion_magnitude = np.linalg.norm(flow[0][0])
                        motion_scores.append(motion_magnitude)
                        timestamps.append(frame_count / fps)
                
                prev_frame = gray
            
            frame_count += 1
        
        cap.release()
        
        # æ‰¾åˆ°è¿åŠ¨å³°å€¼
        if motion_scores:
            motion_scores = np.array(motion_scores)
            mean_motion = np.mean(motion_scores)
            std_motion = np.std(motion_scores)
            
            # æ‰¾åˆ°é«˜äºå¹³å‡å€¼+æ ‡å‡†å·®çš„å³°å€¼ç‚¹
            peaks = np.where(motion_scores > mean_motion + std_motion)[0]
            
            for peak_idx in peaks:
                if peak_idx < len(timestamps):
                    cut_point = OptimalCutPoint(
                        timestamp=timestamps[peak_idx],
                        confidence=0.7,
                        cut_type="action_peak"
                    )
                    cut_points.append(cut_point)
        
        return cut_points
    
    def _select_optimal_segments(self, cut_points: List[OptimalCutPoint],
                               target_duration: float,
                               min_duration: float,
                               max_duration: float) -> List[OptimalCutPoint]:
        """é€‰æ‹©æœ€ä¼˜çš„èµ·å§‹å‰ªè¾‘ç‚¹ï¼ˆç”¨äºè·¨é›†ç»„åˆï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¸å†å¯»æ‰¾å•é›†å†…çš„å®Œæ•´ç‰‡æ®µï¼Œè€Œæ˜¯æ‰¾åˆ°ä¼˜è´¨çš„èµ·å§‹ç‚¹ï¼Œ
        ä¾›åç»­è·¨é›†å¤„ç†ä½¿ç”¨ã€‚å•é›†æ—¶é•¿é€šå¸¸è¿œå°äºç›®æ ‡æ—¶é•¿è¦æ±‚ã€‚
        """
        if not cut_points:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åœºæ™¯å˜åŒ–ç‚¹")
            return []
        
        # æŒ‰æ—¶é—´æ’åº
        cut_points.sort(key=lambda x: x.timestamp)
        
        logger.info(f"åœ¨å•é›†å†…æ‰¾åˆ° {len(cut_points)} ä¸ªåœºæ™¯å˜åŒ–ç‚¹")
        logger.info(f"ğŸ” AIå‰ªè¾‘ç‚¹ç­›é€‰ç­–ç•¥è¯´æ˜:")
        # AIç­›é€‰ç­–ç•¥ï¼šé¿å¼€ç‰‡å¤´ç‰‡å°¾ï¼Œé€‰æ‹©é«˜è´¨é‡å‰ªè¾‘ç‚¹
        
        # é€‰æ‹©ä¼˜è´¨èµ·å§‹ç‚¹çš„ç­–ç•¥ï¼š
        # 1. ä¸è¦å¤ªæ—©ï¼ˆé¿å…ç‰‡å¤´ï¼‰
        # 2. ä¸è¦å¤ªæ™šï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹ä¾›åç»­æ‰©å±•ï¼‰
        # 3. é€‰æ‹©ç½®ä¿¡åº¦é«˜çš„ç‚¹
        
        optimal_start_points = []
        
        # é™é»˜è¯„ä¼°å€™é€‰å‰ªè¾‘ç‚¹
        
        for i, point in enumerate(cut_points):
            # è¿‡æ»¤æ¡ä»¶ï¼š
            # - ä¸è¦å¼€å¤´çš„å‰3ç§’ï¼ˆå¯èƒ½æ˜¯ç‰‡å¤´/å¹¿å‘Šï¼‰
            # - ä¸è¦ç»“å°¾çš„å15ç§’ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹ï¼‰
            # - ç½®ä¿¡åº¦è¦è¶³å¤Ÿé«˜
            meets_criteria = (point.timestamp >= 3.0 and 
                            point.timestamp <= max(cut_points[-1].timestamp - 15.0, 30.0) and
                            point.confidence >= 0.6)
            
            # é™é»˜å¤„ç†å€™é€‰ç‚¹è¯„ä¼°
            if meets_criteria:
                # ä¸ºèµ·å§‹ç‚¹è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå»é™¤è´¨é‡è¯„åˆ†ï¼‰
                # 1. ç½®ä¿¡åº¦è¯„åˆ† (0-1): åŸºäºåœºæ™¯å˜åŒ–å¼ºåº¦
                confidence_score = point.confidence
                
                # 2. æ—¶é—´ä½ç½®åå¥½è¯„åˆ† (0-1): åå¥½30ç§’å·¦å³çš„ä½ç½®
                time_position_score = 1.0 - abs(point.timestamp - 30.0) / 60.0
                time_position_score = max(0.0, min(1.0, time_position_score))
                
                # ç»¼åˆè¯„åˆ† = ç½®ä¿¡åº¦70% + æ—¶é—´ä½ç½®30%ï¼ˆåŸç‰‡è´¨é‡æœ‰ä¿éšœï¼Œæ— éœ€è¯„ä¼°ï¼‰
                overall_score = (
                    confidence_score * 0.7 +     # ç½®ä¿¡åº¦ä¸ºä¸»è¦å› ç´ ï¼ˆåœºæ™¯å˜åŒ–å¼ºåº¦ï¼‰
                    time_position_score * 0.3    # æ—¶é—´ä½ç½®åå¥½ï¼ˆé¿å…å¼€å¤´ç»“å°¾ï¼‰
                )
                
                # åˆ›å»ºä¼˜åŒ–çš„èµ·å§‹ç‚¹
                start_point = OptimalCutPoint(
                    timestamp=point.timestamp,
                    confidence=overall_score,
                    cut_type="optimal_start_point"
                )
                optimal_start_points.append(start_point)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç‚¹ï¼Œé™ä½æ ‡å‡†
        if not optimal_start_points and cut_points:
            logger.warning("ä½¿ç”¨å®½æ¾æ¡ä»¶é‡æ–°é€‰æ‹©èµ·å§‹ç‚¹")
            for point in cut_points:
                if point.timestamp >= 3.0 and point.confidence >= 0.4:
                    start_point = OptimalCutPoint(
                        timestamp=point.timestamp,
                        confidence=point.confidence * 0.8,  # é™ä½è¯„åˆ†è¡¨ç¤ºæ˜¯å¤‡é€‰
                        cut_type="fallback_start_point"
                    )
                    optimal_start_points.append(start_point)
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œåªè¿”å›1ä¸ªæœ€ä½³èµ·å§‹ç‚¹
        optimal_start_points.sort(key=lambda x: x.confidence, reverse=True)
        
        if optimal_start_points:
            best_point = optimal_start_points[0]  # åªå–æœ€ä½³çš„1ä¸ªèµ·å§‹ç‚¹
            logger.info(f"âœ… AIé€‰æ‹©æœ€ä½³èµ·å§‹ç‚¹: {best_point.timestamp:.1f}s")
            return [best_point]
        else:
            logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„èµ·å§‹ç‚¹")
            return []


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_scene_analyzer():
    """æµ‹è¯•åœºæ™¯åˆ†æå™¨"""
    analyzer = SceneAnalyzer()
    
    # è¿™é‡Œéœ€è¦ä¸€ä¸ªçœŸå®çš„è§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    # video_path = Path("test_video.mp4")
    # scenes = analyzer.analyze_video_scenes(video_path)
    # print(f"æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")
    
    print("åœºæ™¯åˆ†æå™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_scene_analyzer()
